18 June
reduce gamma
th main.lua -env BaxterEnv -modelBody BaxterNet -bootstraps 0 -PALpha 0 -steps 200000 -hiddenSize 512 -memSize 50000 -epsilonSteps 50000 -tau 10000 -learnStart 10000 -progFreq 1000 -valFreq 5000 -valSteps 2000 -checkpoint true -batchSize 16 -memSampleFreq 2 -gamma 0.90
=======================
15 June
++ Reward shaping
- small incentive to move closer to object
- + 0.01 for smaller Euclidean dist to obj
- -0.015 for larger Eucl dist to obj (cannot cancel out with back and forth movement)
-> does not over power original scheme

=======================
13,14 June
++Toy runs:
- stationary obj
- smaller range (-0.08 - 0.05)
- small range (-0.01 - 0.08) *no occlusion

- observed very similar conv layers across (and with other runs)
-> not promising

++ check run -> (confirmed it was just luck)
th main.lua -env BaxterEnv -modelBody BaxterNet -bootstraps 0 -PALpha 0 -steps 500000 -hiddenSize 512 -memSize 100000 -epsilonSteps 100000 -tau 10000 -learnStart 10000 -progFreq 1000 -valFreq 5000 -valSteps 2000 -checkpoint true -batchSize 16 -memSampleFreq 2

========================
12 June
++ Completed first proper run (BaxterEnv09-1206)
++ chagne net conv filt size
-> reduced size, larger feat of strate
th main.lua -env BaxterEnv -modelBody BaxterNet -bootstraps 0 -PALpha 0 -steps 500000 -hiddenSize 512 -memSize 100000 -epsilonSteps 100000 -tau 10000 -learnStart 10000 -progFreq 1000 -valFreq 5000 -valSteps 2000 -checkpoint true -batchSize 16 -memSampleFreq 2

=========================
8 June
++ combined velocity and contact detection
- no 0 reward observed
- confirmed reward feed successful

++ Replace ELU with ReLU
++ further limit field of view:
spawning area [-0.1, -0.1] to [0.13, 0.13]
field of view limit [-0.06, -0.06] and [0.12, 0.12]
========================
7 June
- SDK does not have cameraController service
-> need to process image (abandon head camera plan)

- object contact sensor method unreliable (to leave out)
-> depends on timing of gripper motion
-> gripper removed too early / late -> does not publish 
even when 'if' condition satisfied (AND entered)

++ attempt to increase reward for hit
- detect multiple times for object velocity
- detect after every movement for gripping

++ complete implementation of bird-eye camera rack
++ complete bird-eye camera calibration

++ complete cropping of both RGB images

++ for occlusion, skew and increase object spawning area
- [-1, 1] to [-1, 1.3] for both axis
++++++++++++++
Python snippet to print messages from topic

filepath = '/home/hyk13/ros_ws/src/baxter_dqn_ros/somefile'+str(time.time()).split('.')[0]+'.txt'
		with open(filepath, 'w+') as f:
			f.write(str(obj_contact))  # python will convert \n to os.linesep
			f.close() 
		
========================
6 June
- built reward scheme code logic test case

- TODO:
- not using right hand camera
- add bird-eye view RGB camera
Kai: want some sensor info the robot cannot provide itself
- left hand camera occlusion
========================
5 June

++ diagnose reward scheme implementation problem
- verified problem with motion:
-> gripper pressed directly upon sphere
-> sphere no velocity, pick up not complete
-> reward = 0 (instead of -1)
* pick up (reward = 10) most reliably recorded
* hitting / no hit (1, -1) scores becomes unreliable (incorrect reward = 0)
* potential reason: delay in score reporting?

++ in progress: change motion-reward scheme
- add contact sensor to blue sphere
collision2_name: baxter::l_gripper_l_finger::l_gripper_l_finger_collision_l_gripper_l_finger_tip

collision2_name: baxter::l_gripper_r_finger::l_gripper_r_finger_collision_l_gripper_r_finger_tip

TODO:
- fix motion-reward mechanism
- replace depth with 2nd RGB camera (D redundant given left hand camera)
-> right hand camera view from above
- crop left hand camera -> partial occlusion of object in view

***********
Atari code weighing:
- Model.lua (line 141, 144)
- orthogonal weight init + zero bias
-> equal weighing for all possible actions
- add 1 to feature to bias
-> increase weight for action to happen more
* also need to be added in net (after any linear/convolutional layer)

========================
3 June

- switch to left gripper image
- move depth camera

========================
2 June

-- Training with simplification + reward shaping
- only sphere, 3 colours
- no orientation change
- remove rotate_wrist command + left_w2 motor info

========================
1 June

++ Reward shaping
- learnt policy to only pick up

TODO: further simplification option
- only sphere, 3 colours
- no orientation change
- remove rotate_wrist command + left_w2 motor info

========================
31 May

- remove randomness in left arm initial position
- change score spec

TODO:
1. test score adding
- fix object position to directly below arm
- arm only picks up
-> scores should only increase

- allow object small movement range
- arm only picks up
- check probability of picking up object

=======
double check in gazebo
time whole run
th main.lua -env BaxterEnv -modelBody BaxterNet -bootstraps 0 -PALpha 0 -steps 200000 -hiddenSize 512 -memSize 50000 -epsilonSteps 50000 -tau 10000 -learnStart 10000 -progFreq 1000 -valFreq 5000 -valSteps 2000 -checkpoint true -batchSize 16
========================
24th May

++ TRAINING HYPERPARAMS
th main.lua -env BaxterEnv -modelBody BaxterNet -bootstraps 0 -PALpha 0 -steps 200000 -hiddenSize 512 -memSize 50000 -epsilonSteps 50000 -tau 10000 -learnStart 10000 -progFreq 1000 -valFreq 5000 -valSteps 500 -checkpoint true

- steps: ~2 days
=========================
22nd May

- recropped image

- Depth image: no colour representation of deepest colour
--> black becomes white (black)

===========================================================
19th May

++ completed and tested test.lua -> working
- input from DepthMap is 4-channel greyscale
- not 1 channel as expected
- values c [0,1]
++ completed architecture of BaxterNet (1st version)

TODO:
1. Increase info density in image
- redo cropping (both: top, RGB: left (+ right?) D: left + right
- resize (60 - 80 px)
- increase contrast -> decrease depth camera frustum
2. adapt tensor size to 1)
3. Demystify depth image structure
- Rendering difference rivz vs. image.display -> diff image handling methods?
4. Check camera image-taking timing (confirmed with Kai not to investigate for now)

RGB image: 800x800
===========================================================
17 May

++ Speed up test
- run gzserver not gzclient -> set gui to false
- increase ratio of simulation time to real-time -> set real time update rate = 0
- log frequency in Atari: 100 (-progFreq option) (to time)

***REPORT MATERIAL***
+ training speed at 5000 steps
disable gui + ratio: 100 steps/min
disable gui + default ratio: 40 steps/min
enable gui + ratio = 100 steps/min


- Implement net input test method (visualisation)
- adapted test.lua for BaxterEnv (in progress)

TODO: test/debug all written code
++++++++
Setting for gui/headless/use_sim_time etc in
~/ros_ws/src/baxter_simulator/baster_gazebo/launch/baxter_world.launch 
++++++++
Gazebo world physics properties
- The real time update rate parameter specifies in Hz the number of physics updates that will be attempted per second. If this number is set to zero, it will run as fast as it can. Note that the product of real time update rate and max step size represents the target real time factor, or ratio of simulation time to real-time.
- The max step size specifies the time duration in seconds of each physics update step.

<real_time_update_rate> (default = 1000.0) tag in
~/ros_ws/src/baxter_simulator/baster_gazebo/worlds/baxter.world
++++++++
Atari Setup.lua params (selection)
114  cmd:option('-tau', 30000, 'Steps between target net updates Ï„') -- Tuned DDQN target net update interval (3x that of DQN)
126   cmd:option('-learnStart', 50000, 'Number of steps after which learning starts')
129  cmd:option('-progFreq', 10000, 'Interval of steps between reporting progress')
130  cmd:option('-reportWeights', 'false', 'Report weight and weight gradient statistics')
132  cmd:option('-valFreq', 250000, 'Interval of steps between validating agent') -- valFreq steps is used as an epoch, hence #epochs = steps/valFreq
133  cmd:option('-valSteps', 125000, 'Number of steps to use for validation')

===========================================================
16 May 22:34

++ 1st experiment train run
- Tom's code, fixed right arm position (remove randomness added)
- Random factor for get_start_position() = 0.15
- note:
1. original random factor (0.05) limits movement -> for the range of movement arm would not be able to pick up object regardless -> 
2. randomness in right arm removed -> needs increased randomness in left arm
3. training it to move left arm, so randomness should be in left arm?
4. Needs to investigate fissure movement - left forearm bends after attempt then forced back to position for move_vertical() -> can be fixed?

+ training speed: 10000 steps /4hrs -> 42/min
